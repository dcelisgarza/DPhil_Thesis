\chapter{EasyDD v2.0}
\label{c:easydd}

EasyDD can be found in \href{https://github.com/TarletonGroup/EasyDD}{https://github.com/TarletonGroup/EasyDD}.

As mentioned in \cref{s:objectives}, this project aims to integrate multidisciplinary skills for creating increasingly faithful recreations of reality in a user friendly way. This chapter details the software engineering that moved us closer to this goal and yielded a much improved version of EasyDD.

\section{Bug fixes}
\label{s:bugs}

\subsection{Correct time-adaptive integrator}
\label{ss:integrator}

The first obvious hurdle to overcome when making more complex simulations possible was the tremendously long computational time taken for a dislocation plasticity simulation to sufficiently advance past the elastic regime. A simple microcantilever bending simulation with a few frank reed sources would take a whole night to reach the plastic regime, some early simulations by Bruce Bromage took days to get there.

The unacceptable slowness prompted a closer examination of the adaptive-time integration method found in algorithm 10.2 and described by equations (10.42, 10.45 and 10.46) found in \cite[p.~214--216]{ddlab}, an explicit Euler-trapezoid predictor-corrector solver. These are \cref{alg:trapezoid} and \cref{eq:trapezoid},
\begin{align}\label{eq:trapezoid}
    \vec{r}_i^{\rvar{P}}(t + \Delta t) & = \vec{r}_i(t) + \vec{g}_i\left(\left\{ \vec{r}_j(t) \right\}\right) \Delta t\,,                                                                     \\
    \vec{r}_i(t + \Delta t)            & = \vec{r}_i + \dfrac{\vec{g}_i(\left\{\vec{r}_j(t)\right\}) + \vec{g}_i\left(\left\{\vec{r}_i^{\rvar{P}}(t + \Delta t)\right\}\right)}{2}\Delta t\,, \\
    \vec{v}_i                          & \coloneqq \dfrac{\rvar{d}\vec{r}_i}{\rvar{d}t} = \vec{g}_i\left(\left\{\vec{r}_j\right\}\right)\,,
\end{align}
where $\vec{r}_i$ are nodal coordinates, $\vec{v}_i$ are nodal velocities, $\vec{g}_i$ is the function that uses the mobility model and nodal forces to compute the nodal velocities, and the superscript $\rvar{P}$ denotes the predictor. This solver is fast and accurate for a small enough timestep $\Delta t$. Two free parameters $\Delta t_{\rvar{max}}$ and $\epsilon$ denote the maximum allowed timestep and accuracy.
\begin{algorithm}
    \caption{Adaptive Euler-trapezoid predictor-corrector algorithm.}
    \label{alg:trapezoid}
    \begin{enumerate}
        \item Initialise time step $\Delta t \gets \Delta t_{\rvar{max}}$.
        \item $\Delta t_0 \gets \Delta t$.
        \item Compute $\vec{r}_i^{\rvar{P}}(t + \Delta t)$ and corrector $\vec{r}_i(t + \Delta t)$ from \cref{eq:trapezoid}.
        \item If $\max_i\left(\lVert \vec{r}_i^{\rvar{P}}(t + \Delta t) - \vec{r}_i(t + \Delta t)\rVert\right) > \epsilon$, reduce time step $\Delta t \gets \Delta t / 2$ and go to 3.
        \item $t \gets t + \Delta t$.
        \item If $\Delta t = \Delta t_0$, increase time step to $\Delta t \gets \min(1.2 \Delta t, \Delta t_{\rvar{max}})$.
        \item Return to 2, unless total number of cycles is reached.
    \end{enumerate}
\end{algorithm}

The algorithm being used was not exactly this \cref{alg:trapezoid}, as it used a different way of calculating the error but other than that they were the same. Unfortunately, this only increases the time step after converging to a satisfactory answer and advancing the time. So if anything caused the timestep to decrease, such as a collision or dislocation reaction, the time step would only increase very little every subsequent step, thus eading to unecessarily small time steps in most cases. The original implementation of this is found in commit \href{https://github.com/TarletonGroup/EasyDD/blob/780a6c41b35687b443d3241674af7393d2140639/int_trapezoid.m}{\texttt{65907b0}} under the name \texttt{int\_trapezoid.m}. An improved algorithm is described in \cref{alg:trapezoid_improved}. This algorithm takes close to the maximum allowed time step for the maximum allowable accuracy, ($\epsilon_{\rvar{max}}$, $\upsilon_{\rvar{max}}$), maximum and minimum timestep, ($\Delta t_{\rvar{max}}$, $\Delta t_{\rvar{min}}$) and number of iterations, $\rvar{iter}_{\rvar{max}}$. It also uses a better error heuristic and more conservative timestep increase. It can be found in the most current version of \href{https://github.com/TarletonGroup/EasyDD/blob/master/src/int_trapezoid.m}{\texttt{int\_trapezoid.m}}.
\begin{algorithm}
    \caption{Improved adaptive timestep algorithm.}
    \label{alg:trapezoid_improved}
    \begin{algorithmic}
        \State $\rvar{Convergent} \gets \rvar{false}$, $\Delta t_{\rvar{valid}} \gets 0$, $\rvar{iter} \gets 0$, $\rvar{flag} \gets \rvar{false}$
        \While{$\rvar{Convergent}$}
        \State Compute $\vec{r}_i^{\rvar{P}}(t + \Delta t)$ and corrector $\vec{r}_i(t + \Delta t)$ from \cref{eq:trapezoid}.
        \State $\Delta \vec{r}_i \gets \vec{r}_i(t + \Delta t) - \vec{r}_i^{\rvar{P}}(t + \Delta t)$
        \State $\vec{\overline{r}}_i \gets \dfrac{\vec{g}_i(\left\{\vec{r}_j(t)\right\}) + \vec{g}_i\left(\left\{\vec{r}_i^{\rvar{P}}(t + \Delta t)\right\}\right)}{2}\Delta t$
        \State $\epsilon \gets \max_i\left(\lVert \Delta \vec{r}_i \rVert \right)$
        \State $\upsilon \gets \max_i\left(\lVert \Delta \vec{r}_i - \vec{\overline{r}}_i \rVert\right)$
        \If{$\epsilon < \epsilon_{\rvar{max}} $ and $\upsilon < \upsilon_{\rvar{max}}$}
        \State $\Delta t_{\textrm{valid}} = \Delta t$
        \State $\gamma \gets 1.2\left(\dfrac{1}{\left[1 + (1.2^{20} - 1) (\epsilon / \epsilon_{\textrm{max}})\right]}\right)^{1/20}$
        \State $\rvar{flag} \gets \rvar{true}$
        \State $\rvar{iter} \gets \rvar{iter} + 1$
        \State $\Delta t \gets \max\left(\gamma \Delta t,\, \Delta t_\rvar{max}\right)$
        \Else
        \If{$\rvar{flag} == \rvar{true}$}
        \State $\Delta t \gets \Delta t_{\rvar{valid}}$
        \State $\rvar{iter} \gets \rvar{iter}_\rvar{max}$
        \Else
        \State $\Delta t \gets \Delta t / 2$
        \EndIf
        \EndIf
        \If{$\rvar{iter} > \rvar{iter}_{\rvar{max}}$ or $\Delta t == \Delta t_{\rvar{max}}$ or $\Delta t < \Delta t_\rvar{min}$}
        \State $\rvar{Convergent} \gets \rvar{true}$
        \EndIf
        \EndWhile
        \State $t \gets t + \Delta t$
        \State Proceed with the rest of the simulation.
    \end{algorithmic}
\end{algorithm}

\Cref{alg:trapezoid_improved} is more computationally expensive per iteration, but can increase the timestep much more rapidly than \cref{alg:trapezoid} without going through the rest of the simulation, which involves computationally expensive procedures. This improvement by itself let the early version of the software move through the elastic regime of our simulations orders of magnitude times faster than before. Simulations that took hours to days to reach the plastic regime, started doing so in minutes to hours. It also narrowed the gap between using different loading conditions, as the simulations could now effectively adjust the timestep to match the error tolerances much more easily than before. Both of which were a great boon to the usability of the code and to the research group's output capacity. This was the first major bottleneck to be fixed, which allowed us to identify and resolve previously unknown issues downstream.

\subsection{Matrix conditioning}
\label{ss:matrix}

A common problem with dislocation dynamics is the tendancy for some networks to have a large degree of variation in the speed of dislocation nodes \cite{bertin2019gpu,ddlab,arsenlis2007enabling}. This is a consequence of the fact that in discrete dislocation dynamics, nodes are simply the discretised representation of a dislocation line. As such, nodal motion must be constrained to the motion of dislocations, i.e. nodal movement must be consistent with how a dislocation glides, climbs and cross lips accounting for the dislocation character and orientation in the crystal. Moreover, dislocation motion can be assumed to follow a linear, anisotropic viscous drag model \cite{ddlab},
\begin{align}\label{eq:dragCoef}
    \vec{F}_\textrm{drag}(\vec{x}) & = -\mathcal{B}(\vec{\xi}(\vec{x})) \cdot \vec{v}(\vec{x})\,,
\end{align}
where $\mathcal{B}$ is a tensor constructed from the anisotropic drag that a dislocation node, $\vec{x}$, experiences as a result of its constrained velocity $\vec{v}(\vec{x})$ due to being part of a discritesed segment $\vec{\xi}(\vec{x})$. In simple terms, this means a node experiences very different resitances to moving along its allowed directions.

Different mobility laws have different parametrisations for these directions. A problem that often plagues traditional mobility laws\footnote{Bruce Bromage, of \cite{bromage2018calculating} has developed a new BCC mobility law, \href{https://github.com/TarletonGroup/EasyDD/blob/master/src/mobbcc_bb1b.m}{\texttt{mobbcc1\_bb.m}} which can be found in the most current version of EasyDD. It solves many of the issues with traditional laws and is our new go-to for BCC materials. Among the things it fixes are: unreasonable cross-slip, pencil glide, and the problematic line-direction parametrisation. The drag matrix, $\mtx{B}$, can still be singular however, so this law also includes the scale-averaged Marquardt-Levenberg regularisation.} is the movement of a node along a line segment itself. Having a node move along a single line segment has no effect on the energy of a network, as seen in \cref{fig:lineMovement}. Both structures are equivalent because the line---and thus the dislocation---does not change from one to the other, merely the limits of the line integral change. Therefore nodal movement along a line should not contribute to the total drag, and therefore have a zero line drag coefficient. On the other end of the spectrum we have climb, which is orders of magnitude less favourable than glide. But this can make \cref{eq:dragMatrix} singular, so its value is the careful balancing act of keeping the matrix invertible whilst keeping the associated energy cost of moving along a line low. This also causes nodal speeds along line directions to be much higher than other directions, which often limits the step size an integrator can take.
\begin{figure}\label{fig:lineMovement}
    \centering
    \includegraphics[width=0.5\linewidth]{lineDirMov.pdf}
    \caption{Node movement along dislocation line should have no drag contribution.}
\end{figure}

In the overdamped regime, $\dfrac{\rvar{d}\vec{v}}{\rvar{d}t}(\vec{x}) \to \vec{0}$, known driving forces (Peach-K\"{o}hler force + segment-segment forces + self-force) and the unkown drag force sum to give,
\begin{align}\label{eq:drivePlusDrag}
    \vec{F}_\rvar{drag}(\vec{x}) + \vec{F}_\rvar{drive}(\vec{x}) = \vec{0}\,.
\end{align}
Substituting \cref{eq:dragCoef} into \cref{eq:drivePlusDrag} and rearranging gives,
\begin{align}\label{eq:nodeMob}
    \mathcal{B}(\vec{\xi}(\vec{x})) \cdot \vec{v}(\vec{x}) & = \vec{F}_\rvar{drive}(\vec{x})\,.
\end{align}
Strictly speaking, solving \cref{eq:dragCoef} requires global knowledge of the network. The discretisation makes it possible to distribute $\mathcal{B}$ along a line segment $i-j$,
\begin{align}\label{eq:dragMatrix}
    \mtx{B}_{ij} & \gets \oint_{C} N_i(\vec{x}) \mathcal{B}(\vec{\xi}(\vec{x})) N_j(\vec{x}) \rvar{d}L\,,
\end{align}
where $C$ is the whole network, $N_i$ and $N_j$ are simply shape functions that interpolate quantities given the relative position of a node to a segment, and $\rvar{d}L$ is the infinitesimal line segment. Thus giving individual expressions for node $i$ connected to node $j$. Using this discretisation and assuming the nodes connected to $i$ are subject to similar conditions, \cref{eq:nodeMob} can be broken up into local segments,
\begin{align}\label{eq:nodeVel}
    \vec{v}_i \approx \left(\sum\limits_j \mtx{B}_{ij}\right)^{-1} \cdot \vec{f}_i,\,
\end{align}
where $i$ is the node in question, $j$ are the nodes it shares a segment with, and $\vec{f}_i$ is the local force on the node. From now on, we will drop the Einstein notation and refer to the local frame as $\vec{v} = \mtx{B}^{-1} \vec{f}$.

The immediate implication of the orders of magnitude difference between the drag coefficients used to construct $\mathcal{B}$---and therefore $\mtx{B}$---means the condition number of $\sum_j \mtx{B}$ can be very large. As $\mtx{B}$ is symmetric and therefore normal, its condition number is given by,
\begin{align}\label{eq:conditionNumb}
    \kappa(\mtx{B}) = \dfrac{\lvert \lambda_\rvar{max} \rvert}{\lvert \lambda_\rvar{min} \rvert} \,,
\end{align}
where $\lambda_i$ is the $i$\textsuperscript{th} eigenvalue.

The condition number of a matrix dictates how much collinearity exists in its basis. The larger the condition number, the larger the collinearity and the more sensitive the sytem is to small perturbations. Such systems are said to be ill-conditioned. As a general rule, a condition number $\kappa(\mtx{A}) = 10^k$ represents a loss of up to $k$ digits of precision on top of what is already lost from arithmetic methods under limited precision \cite{cheney2012numerical}.

True to Murphy's law, $\mtx{B}$ is frequently ill-conditioned---even in the simplest simulations. The way these cases were handled relied on the integrator reducing the timestep enough to sidestep the issue. We call this ``strongly coupled'' behaviour. Strong coupling between functionally independent functions is a \emph{very} bad practice in software engineering.

A function $\mathcal{X}$ with a set of $\mathbb{X}$ bugs with phase space $\chi(x)$ that is, $\forall x \in \mathbb{X}~ \chi(x)$. Bugs in production code are often either low impact with broad phase spaces, or high impact with narrow phase spaces. Both of which can be avoided with a combination of proper software design and testing protocols. All this goes out the window when functions are not properly decoupled. The multiplicative product of the corresponding phase spaces is often non-trivial, and can sometimes cancel out in some cases and blow up in others. Complicating the diagnosis and solution of errors.

The way the mobility functions dealt with ill-conditioned $\mtx{B}$, relied on the integrator picking up the slack. There are several flaws with the approach found in \cref{alg:bTotalOld}, the actual mobility function can also be found under commit \href{https://github.com/TarletonGroup/EasyDD/blob/65907b022d1fe408fc1b2e5c5ca2bd1797ccae04/mobbcc1.m#L68}{\texttt{65907b0}} under the name \texttt{mobbcc1.m}. Every mobility law in that commit has this issue.
\begin{algorithm}
    \caption[Bad way of avoiding drag matrix inversion singularity.]{Avoiding singular matrix by making $\mtx{B}$ extremely wrong and hoping the integrator error bounds pick it up and the timestep is decreased.}
    \label{alg:bTotalOld}
    \begin{algorithmic}
        \State Compute eigenvalue matrix, $\mtx{D} \gets \mtx{P}^{-1} \mtx{B} \mtx{P}$
        \State $\kappa(\mtx{B}) \gets \lvert \lambda_{\rvar{max}} \rvert/ \lvert \lambda_{\rvar{min}} \rvert$
        \If{$\kappa(\mtx{B}) < \kappa_{\rvar{min}}$}
        \State $\mtx{D}_{\rvar{norm}} \gets \mtx{D} / \lambda_{\rvar{max}}$
        \State $\vec{f} \gets \vec{f} / \lambda_{\rvar{max}}$
        \State{\Comment{Invert $\mtx{D}_{\rvar{norm}}$.}}
        \ForAll{$(\lambda_k \in \mtx{D}_{\rvar{norm}}) \neq \lambda_{\rvar{max}}$}
        \If{$\lambda_k > \lambda_{\rvar{crit}}$}
        \State $\lambda_k \gets 1/\lambda_k$
        \Else
        \State{\Comment{The inverse of 0 is not 0.}}
        \State $\lambda_k \gets 0$
        \EndIf
        \EndFor
        \State $\vec{v} = \mtx{P} \mtx{D} \mtx{P}^{-1} \vec{f}$
        \Else
        \State $\vec{v} = \mtx{B}^{-1} \vec{f}$
        \EndIf
    \end{algorithmic}
\end{algorithm}

The first is assuming that an ill-conditioned matrix can be fixed by diagonalising. This is simply not true, perturbing the smallest eigenvalue in a direction such that $\kappa$ improves is a good way of doing so without adding much error. However, perturbations of this type are often not enough when the condition number is too large. Furthermore, one wants to keep the overall behaviour of the system, i.e. maintain the relative sizes of the eigenvalues with respect to one another. If the multiple eigenvalues are close in magnitude to the smallest eigenvalue, one can change the dynamics of the system if the perturbation is too large. Not to mention the fact that $\kappa$ may still be too large. In a system as stiff and dynamic as discrete dislocation dynamics, this approach is not enough for some cases.

The second flaw is the assumption that the case where $\lambda_k < \lambda_{\rvar{crit}}$ for non-maximal eigenvalues, is so rare, it won't have much of an effect. Unfortunately, it's common in junctions where nodal mobility is limited, as well as in volumes with large stress gradients. Although to be fair, these are also consequence of the assumption of locality, yielding \cref{eq:nodeVel}. Moreover, when every non-maximal eigenvalue is smaller than $\lambda_{\rvar{crit}}$, the error much greater. For a every eigenvalue under the threshold of of $\lambda_{\rvar{crit}} = 10^{-k}$, the upper bound of the error is $k$ digits in the corresponding mobility component.

The third is the assumption that the integrator will not allow obviously wrong solutions because it will fail the error check and reduce the time step. This assumption breaks down whenever dislocation velocities are calculated outside the context of time integration, such as every topological operation where a new node is spawned.

The regularisation of ill-conditioned systems in an active field of research \cite{regularisation1,regularisation2,regularisation3}. However, they require some knowledge of the matrix structure or follow a well-known statistical distribution to thier values/eigenvalues. Regularlisation is most often applied to high rank matrices because poorly conditioned low rank matrices ones are often very sensitive to perturbations, thus requiring specialised methods. In this case, $\mtx{B}$ is of rank equal to the number of orthogonal crystallographic directions, so it is always of low rank.

One of the most effective regularisations for low rank matrices is dampening the inversion by adding a small value, $c$---called the Marquardt-Levenberg coefficient---to the diagonal of the matrix. This is roughly equivalent to perturbing the smallest eigenvalue as previously mentioned. Luckily, if this factor is added the whole diagonal, the inversion can be dampened sufficiently whilst keeping the main contributions from each basis roughly equal in relation to each other. This amounts to adding a multiple of the identity matrix to an ill-conditioned matrix. For a more accurate solution, one can iteratively refine $c$ to find the smallest value that keeps the matrix inversion from blowing up. A good heuristic for the value of the Marquardt-Levenberg coefficient is,
\begin{align}\label{eq:marquardt}
    c = \max\left(\lvert\mtx{A}\rvert\right) \sqrt(\epsilon)\,,
\end{align}
where $\epsilon$ is machine precision. This ensures the perturbation is scaled to a number that will not underflow during arithmetic operations. Simply put, it ensures the perturbation is small but does not disappear during matrix inversion.

It is worth noting that doing this and inverting is still a source of error, adding noise even if small and along the diagonal, does move the system away from its true behaviour. However, the degree to which it is wrong is many orders of magnitude smaller than what is done in \cref{alg:bTotalOld}. Indeed, \cref{alg:bTotalNew} uses $\pm c $ and both perturbed solutions are averaged to give a much more accurate and simpler solution even under the traditionally problematic scenarios discussed above. This new algorithm is now present in the mobility laws we now use, such as the most current version of \href{https://github.com/TarletonGroup/EasyDD/blob/372984499dd60136fc7badabd6cee192058d55d9/src/mobbcc_bb1b.m#L199}{\texttt{mobbcc\_bb.m}}.
\begin{algorithm}
    \caption[Dampening the drag matrix inversion singularity.]{Improved regularisation of $\mtx{B}$ by way of perturbing the diagonal.}
    \label{alg:bTotalNew}
    \begin{algorithmic}
        \If{$\lVert \mtx{B} \rVert < \epsilon$}
        \State $\vec{v} \gets \vec{0}$
        \ElsIf{$\lvert \lambda_{\rvar{max}} \rvert/ \lvert \lambda_{\rvar{min}} \rvert < \epsilon$}
        \State $\mtx{B}_+ \gets \mtx{B} + \sqrt{\epsilon} \max(\mtx{B})$
        \State $\mtx{B}_- \gets \mtx{B} - \sqrt{\epsilon} \max(\mtx{B})$
        \State $\vec{v}_+ \gets \mtx{B}_+^{-1} \vec{f}$
        \State $\vec{v}_- \gets \mtx{B}_-^{-1} \vec{f}$
        \State $\vec{v} \gets (\vec{v}_- + \vec{v}_+)/2$
        \Else
        \State $\vec{v} \gets \mtx{B}^{-1} \vec{f}$
        \EndIf
    \end{algorithmic}
\end{algorithm}

This change enabled other team members, namely Haiyang Yu and Fengxian Liu to get further with their simulations. Haiyang's nanoindentation simulations have particularly complex dislocation structures with a substantial amount of junction formation; while Fengxian's have large localised stress gradients as a result of inclusions. Both require nodal velocieties to be accurately calculated, else the dislocations move erratically, readily cross slip, and cause massive slowdowns as simulations advance.

\section{Research software engineering}
\label{s:rse}

Many scientists and researchers spare no thought for good software. However, with the presently on-going SARS-CoV 2 pandemic, the importance of research software has been made clear. Modelling has played a large role in informing government policy in the UK and the world \cite{covidScotland,covidUK1,covidUK2}. With modelling thrust into the spotlight and the public release of the Imperial College modelling code used to inform so much policy \cite{covidUK2}, \href{https://github.com/mrc-ide/covid-sim}{\texttt{https://github.com/mrc-ide/covid-sim}}, criticism was levied at the lack of care taken to ensure the correctness of the code \cite{natureModelCritique}. In experimental research, a lot of care is take to ensure a methodoogy used is reproducible, robust and sound. There are standards, both external and internal, which ensure the quality of the results. But even with these measures in place, there is already a common issue with irreproducibility in science \cite{mede2020replication,randall2018irreproducibility,bolli2015reflections}. In obtaining results and analysing them, software is ubiquitous. It should be good, \emph{especially} if the science is modelling-based.

Along this vein, one of the primary goals of this project was to develop a competent codebase that non-experts can take advantage of with minimal preparation. There were multiple sub-objectives that help us achieve this.
\begin{enumerate}
    \item Provide a generic framework upon which the software can be modularly expanded with minimal change to the source: prevents increasingly divergent and incompatible source code between researchers.
    \item Provide an easy way to autocomplete inputs with sensible default parameters: prevents the software from crashing when---inevitably---one of the required arguments for a function is undefined.
    \item Improve readability and organisation: makes it easy to identify, fix bugs and know what the code does.
    \item Compartmentalise variables: makes the code more generic and increases usability and readability by reducing the number of function arguments.
    \item Optimise memory and computation: improves computational speed and reduces resource use.
\end{enumerate}

Akin to the Ship of Theseus, it is hard to tell whether the new version of the code is the same code as when this project began. While \href{https://github.com/TarletonGroup/EasyDD}{\texttt{EasyDD v2.0}} is ``complete'', the process of improving the code is on-going. The amount of initial technical debt since its inception as the infinite-domain discrete dislocation dynamics code, \href{http://micro.stanford.edu/wiki/Main_Page}{\texttt{DDLab}} \cite{ddlab}, plus what it accrued as the Tarleton Group expanded its functionality and turned it into \href{https://github.com/TarletonGroup/EasyDD/tree/65907b022d1fe408fc1b2e5c5ca2bd1797ccae04}{\texttt{EasyDD}}, is quite large and needs to be carefully chipped away so as not to introduce regressions. However, while the quality and capabilities of the code can be greatly improved, there is only so much that can be done without fundamentally redesigning it completely. This is in part the subject of \cref{c:future}.

Other team members have made significant contributions to the release of \href{https://github.com/TarletonGroup/EasyDD}{\texttt{EasyDD v2.0}}. Some of which remain to be added, but should slowly make their way in. Given the code is the amalgam of the research group's work, we will credit other members' contributions where relevant but will leave the details for them to explain.

This is actively evolving on research software. Therefore the work described herein will continue past the end of this project, for there are still many improvements to be made to the old codebase on top of whatever functionality is added by future researchers.

\subsection{Organisation}

An oft-overlooked aspect of code useability is the organisational aspect. However as a codebase grows, it becomes more important for both a code be properly organised and source controlled. This makes it easier to browse, understand, is less overwhelming for users and developers and crucially provides a safey blanket against things going wrong.

\subsubsection{Source control}

Source control is a crucial aspect of industrial software development. It is an industry standard and the first thing to set up when starting a software development project. Academia is very far behind in this regard. Not only does it provide a history of restore points in case things break; but keeps a history of changes, who and when they made them; can perform automated checks such as testing or conflict detection; lets developers and users (if the repository is open-sourced) make changes independently and request their changes be added to the main code; among many other tasks.

In regards to reproducibility, source control is a crucial tool. In both \cref{ss:integrator,ss:matrix} we took advantage of this to provide a specific commit and a file name, which lets anyone with access to a repository, the ability to see the entire history of the code. In the digital version of this document, those link commits are links directly to the file---and where relevant---specific lines. The advantages of such a system are obvious. For example, when submitting a manuscript for publication one can also submit a link to the specific version of the software used to obtain the results. This is a gigantic stride towards open, reproducible science.

\subsubsection{Folder structure}

An on-going task is improving the folder structure. Which in the past was almost non-existant. Having a default place to place inputs, outputs and source code is of great import if a code is to be used by non-experts. It is trivial to do and a boon to the quality of life of developers and users.

\subsection{Modularisation}

Modern software practices usually make heavy use of some form of variable encapsulation and code modulirisation to make code safer, simpler to expand and easier to use. This enables users and developers to take advantage of a set of toolboxes that can be chosen according to their needs without having to directly modify the source code, this concept is called ``moduliration''.

\subsubsection{Encapsulation}

The act of keeping variables neatly stored in a way that fits a purpose is called encapsulation. By encapsulating variableswe greatly improve the usability of software by increasing readability, reducingthe risk of human error by reducing the number of things to keep track of, and if done properly can even improve performance. This concept is tightly bound to the design philosophy of software as described in the following list.
\begin{itemize}
    \item Object-Oriented Programming (OOP): where variables are viewed as objects upon which the logic acts. Functions can take these objects and operate on them. This design pattern keeps related variables as part of a single entity and reduces the number of things users and developers have to keep track of, thus reducing the chance of implementation and user error. There are many advanced concepts and pitfalls in OOP that unecessarily increase complexity, so it is best to apply the KISS (keep it simple, stupid) approach for best results.
    \item Data-Oriven Programming (DDP): where the data dictates the structure of the programme. It is essentially a modified version of OOP where the object is made to fit the data, not the data made to fit the object. It often leads to more performant code and avoids many of the issues that arise from OOP because it forces developers to really think about how to manage the data.
    \item Functional programming: where everything is a function. There is no state, but also no side effects. This is the strongest form of encapsulation but also the most inflexible.
    \item Procedural programming: this is the most ancient form of programming where there are only functions and variables. It is the form most scientific software packages take and it is the most difficult to work with both, as a developer, user. Even modern computers which can make use of branch prediction and deletion, cache pre-fetching, and compiler optimisations can be hindered by this approach.
\end{itemize}

Procedural programming is what gives us so-called spagghetti code\footnote{Code with many branches and disparate procedures without a clear purpose. It is difficult to work with as a user and developer. It also tends to interfere with branch prediction and deletion as well as compiler optimisations.}. Unfortunately, education in programming or software engineering in science is sorely lacking. As a result, a large amount of scientific software tends to be written in this way (particularly legacy software). Thus we strive to separate ourselves from it as much as possible.

Functional programming is impractical for most use cases so it is seldom used outside of computer science and very specific industrial and academic applications \cite{haskell,functionalProg}.

Object-oriented programming can be very useful if used properly. But it has the potential to unecessarily increase complexity and reduce performance. Fortunately, \mintinline{matlab}{MATLAB} does not truly have object-oriented capabilities, so it forces either a procedural or data-driven approach. Unfortunately once more, it leads to software being written in a procedural manner.

This can be remedied by using \mintinline{matlab}{struct} and being sensible about, i.e. by taking a data-driven approach, in building them. While they are more of an ordered dictionary (Hash table), than a true object, they are sufficient for our use-case. They have allowed us to make use of generic functions for different mobility functions, loading conditions, finite element meshes, the dislocation network itself, etc.All of which have a massively simplifying effect on the code without sacrifing performance. While this project pioneered this task, Daniel Hortelano-Roig has been doing much in the way of increasing its scope. His changes greatly simplify the code and will slowly be ported over to the main branch.

\subsubsection{Generic functions}

Generic functions are implementation-agnostic functions that can be given by the user as any other variable. Only the variable in this case is a function. \mintinline{matlab}{MATLAB} has an old method of doing so using \mintinline{matlab}{feval()}, where the first argument is the name of the file whose function is being called, and subsequent arguments are passed on to the function. However this has some limitations, mainly their performance, and function handles are now preferred, e.g.
\begin{minted}[
    frame=lines,
    linenos
    ]{matlab}
    % myFunction is defined in 'myFunction.m' and is called 
    % like so myFunction(foo, bar)
    genericFunction = 'myFunction.m'; 
    foo = 1;
    bar = 2;
    feval('genericFunction', foo, bar);
\end{minted}
as opposed to,
\begin{minted}[
    frame=lines,
    linenos
    ]{matlab}
    % myFunction is defined in 'myFunction.m' and is called 
    % like so myFunction(foo, bar)
    genericFunction = @myFunction; 
    foo = 1;
    bar = 2;
    genericFunction(foo, bar);
\end{minted}
which are not dissimilar, but the latter is more performant and easier to follow. Generic functions were already in use for dislocation mobility but there are now ones for calculating numeric and analytic tractions (see \cref{c:tractions}), loading conditions (multi-stage loading, constant loading, cyclic loading), various post-processing functions, boundary conditions and various miscelaneous functions that support different simulation types.

The main advantage of generic functions is reducing the need for direct source code modification. Changing source code, especially when there are no automated tests to speak of, is a dangerous proposition. It greatly increases the probability of introducing regressions (new bugs) and code divergeance between researchers. Regressions are always problematic, but code divergeance is a big issue that still affects the Tarleton Group. If experts within the same research group find it difficult to incorporate each others' additions to our work, there is little hope for the general user.

The introduction of regressions makes it so code must be thoroughly and exhaustively tested before merging two people's work. The use of generic functions makes it so if there is a problem with a new addition, the problem is localised and can be quickly identified and fixed because it is isolated. We can be sure the changes are only found in the new piece of code. And since it is generic, we can chose whether or not to use it by changing an input file rather than messing with the source code.

If on the other hand, one were to add a new branch to an \texttt{if} statement (or a brand new \texttt{if} statement) with new computations, likely the function needs new arguments to cover this new case, we may or may not need to make up- or downstream modifications so the function can accommodate the additions. Then very quickly we will find ourselves procedurally programming and fixing the innevitable problems that come with hard to understand spaghetti code---which is most likely also less performant now than it used to be. The problem worsens as we add more functionality. Which can all be avoided by intelligently designing a generic approach, that can be easily expanded if the need arises.

\subsection{Optimisation}
% \renewcommand{\epigraphwidth}{0.9\linewidth}
% \renewcommand{\textflush}{flushright}
% \renewcommand{\epigraphflush}{flushright}
\renewcommand{\epigraphflush}{flushright}
\renewcommand{\textflush}{flushright}
\setlength{\epigraphwidth}{0.75\linewidth}
\epigraph{We \emph{should} forget about small efficiencies, say about 97\% of the time: pre-mature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3\%.}{\textit{--- Donald E. Knuth \cite[p.~268]{knuth1974structured}}}

\subsubsection{Spurious memory allocation}

\mintinline{matlab}{MATLAB} is infamous for the way it will happily dynamically grow an array when going out of bounds. This \emph{terrible} practice is even fundmental to the operation of \texttt{EasyDD}. However, changing this fundamental part of the codebase requires a complete rewrite and not deemed worthwhile, there are worse bottlenecks. However, there were many instances of memory reallocated or arrays grown every cycle that were completely unecessary.

For example, the amount of memory unecessarily reallocated in the function that couples DDD and FEM was $12(N_x + 1)(N_y + 1)(N_z + 1)$ double precision floating point numbers, where $N_i$ is the number of elements in dimension $i$ of the finite domain. The cubic scaling is a performance killer at larger mesh sizes. In a simulation with hundreds or thousands of steps and a mesh with a few thousand FE nodes, the cost added up.

It is not only the cost of allocating, but also the cost of garbage collection \cite{hanson1990fast} i.e. automatically freeing memory based on a set of criteria can be significantly higher. Timing the equivalent of allocating a $20 \times 20 \times 20$ mesh in \mintinline{matlab}{MATLAB 2020b} takes on the order of \SI{70}{\micro\sec} (for x86 CPU architectures), which is in-line to what it costs in \mintinline{C}{C}. Deallocating memory takes about $5 \times$ longer. In a function that otherwise takes a few \si{\milli\sec}, adding another \SI{400}{\micro\sec} of spurious allocation is a significant overhead that can be easily remedied.


\subsubsection{Finite element optimisation}

The use of sparse arrays is a must when working with finite elements, where matrices often take structured sparse forms will significantly reduce memory footprint and computational expense. The code already used these special data structures and Cholesky factorisation for faster solution of linear systems. However, \mintinline{matlab}{MATLAB} has a special form of it that exploits sparsity. This reduced the time required for the factorisation by around $50\times$, reduced the memory footprint of the final sparse arrays by a similar amount, and reduced the memory footprint of the actual factorisation procedure by approximately $10 \times$. The whole point of this factorisation is to make solving linear systems faster, in this case the calculation of the corrective displacements, $\vec{\hat{u}}$ on the free boundaries, $S_U$ of freedom as a result of the forces acting upon them,
\begin{align}
    \vec{\hat{u}} = \mtx{K}^{-1} \vec{f}\quad \text{on } S_U
\end{align}

It improved solution speed by a factor of 3, thus reducing the cost of coupling DDD to FEM to be dominated by the computation of dislocation-induced tractions and displacements.

The final two optimisations also deal with memory. The first, removes unecessary allocations in the calculation of the global stiffness matrix and the introduction of a reduced stiffness matrix such that it only includes relevant degrees of freedom.

In aggregate these changes have given us the ability to use $15 \times$ more nodes as well as letting us build meshes over two orders of magnitude faster, mostly down to improved memory access patterns inside the mesh building loops.

\subsection{Misc quality of life improvements}

Formerly, initialising a simulation would involve a series of non-obvious steps, particularly when initially compiling the C and CUDA code used for accelerating simulations. This is now done automatically on a need-to-compile basis and has a fallback in case no \mintinline{CUDA}{CUDA} compiler is found.

There was also a recurring problem with simulations suddenly failing when a function was called with an undefined argument. This extremely common scenario is unbecoming of good software, especially if the failure happened a few minutes after starting a simulation. This has been remedied with the automatic calculation of a set of reasonable defaults for each and every undefined variable, aside from the arrays defining a dislocation network. Any new developments get added to this very simple, yet much needed script.

\subsection{Summary}

Scientific modelling is inherently an interdisciplinary skill. EasyDD is now faster, more efficient and more capable than ever before, in large part thanks to the work---ongoing or otherwise---described within this chapter. Subsequent chapters explore aspects more specific to discrete dislocation dynamics. However, it is important not to overlook the work that enables the obtention of results. Modern academia deems the final result to be paramount. But what is a result if it cannot be scrutinised because nobody can understand how it came about? Moreover, by developing good software, we allow others to partake in the obtention of results. We deem this to be one of the main contributions of the present project. We feel safe our ability to claim that we now have the capacity for providing more faithful recreations of reality in a less user-unfriendly way. And without so much as a glancing touch on dislocation dynamics.
% 4764 words