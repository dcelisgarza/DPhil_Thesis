\chapter{EasyDD v2.0}
\label{c:easydd}

EasyDD can be found in \href{https://github.com/TarletonGroup/EasyDD}{\texttt{https://github.com/TarletonGroup/EasyDD}}.

As mentioned in \cref{s:objectives}, this project aims to integrate multidisciplinary skills for creating increasingly faithful recreations of reality in a user-friendly way. This chapter details the software engineering that moved us closer to this goal and yielded a much improved version of EasyDD.

\section{Bug fixes}
\label{s:bugs}

\subsection{Correct time-adaptive integrator}
\label{ss:integrator}

The first obvious hurdle to overcome when making more complex simulations possible was the unreasonably long computational time taken for a dislocation plasticity simulation to sufficiently advance past the elastic regime. A simple microcantilever bending simulation with a few frank reed sources would take a whole night to reach the plastic regime, some early simulations by Bruce Bromage took days to get there.

The unacceptable slowness prompted a closer examination of the adaptive-time Euler-trapezoid predictor-corrector solver described in algorithm 10.2 and equations (10.42, 10.45 and 10.46) in \cite[p.~214--216]{ddlab}, reproduced here by \cref{alg:trapezoid,eq:trapezoid},
\begin{align}\label{eq:trapezoid}
    \vec{r}_i^{\rvar{P}}(t + \Delta t) & = \vec{r}_i(t) + \vec{g}_i\left(\left\{ \vec{r}_j(t) \right\}\right) \Delta t\,,                                                                     \\
    \vec{r}_i(t + \Delta t)            & = \vec{r}_i + \dfrac{\vec{g}_i(\left\{\vec{r}_j(t)\right\}) + \vec{g}_i\left(\left\{\vec{r}_i^{\rvar{P}}(t + \Delta t)\right\}\right)}{2}\Delta t\,, \\
    \vec{v}_i                          & \coloneqq \dfrac{\rvar{d}\vec{r}_i}{\rvar{d}t} = \vec{g}_i\left(\left\{\vec{r}_j\right\}\right)\,,
\end{align}
where $\vec{r}_i$ are nodal coordinates, $\vec{v}_i$ are nodal velocities, $\vec{g}_i$ is the function that uses the mobility model and nodal forces to compute the nodal velocities, and the superscript $\rvar{P}$ denotes the predictor. This solver is fast and accurate for a small enough timestep $\Delta t$. The algorithm contains two free parameters, $\Delta t_{\rvar{max}}$ and $\epsilon$, which respectively denote the maximum allowed timestep and accuracy.
\begin{algorithm}
    \caption{Adaptive Euler-trapezoid predictor-corrector algorithm.}
    \label{alg:trapezoid}
    \begin{enumerate}
        \item Initialise time step $\Delta t \gets \Delta t_{\rvar{max}}$.
        \item $\Delta t_0 \gets \Delta t$.
        \item Compute $\vec{r}_i^{\rvar{P}}(t + \Delta t)$ and corrector $\vec{r}_i(t + \Delta t)$ from \cref{eq:trapezoid}.
        \item If $\max_i\left(\lVert \vec{r}_i^{\rvar{P}}(t + \Delta t) - \vec{r}_i(t + \Delta t)\rVert\right) > \epsilon$, reduce time step $\Delta t \gets \Delta t / 2$ and go to 3.
        \item $t \gets t + \Delta t$.
        \item If $\Delta t = \Delta t_0$, increase time step to $\Delta t \gets \min(1.2 \Delta t, \Delta t_{\rvar{max}})$.
        \item Return to 2, unless total number of cycles is reached.
    \end{enumerate}
\end{algorithm}

The original, supposed implementation of this is found in commit \href{https://github.com/TarletonGroup/EasyDD/blob/780a6c41b35687b443d3241674af7393d2140639/int_trapezoid.m}{\texttt{65907b0}} under the name \texttt{int\_trapezoid.m}.

However, this was not the case. There were a number of problems with the implementation. Firstly, it only used $\Delta t_\rvar{max}$ only on the first timestep. It also only increased the timestep once every timestep. Ideally, an adaptive time algorithm should increase the time step to the maximum that is still under the error criteria. The way the old version of the integrator worked meant that the time step would take multiple steps to ramp back up after decreasing. Take for example, a case where the time step had to decrease once for two consecutive steps, $\Delta t = \Delta t_0/4$ would have taken $\lceil\log_1.2(4)\rceil = 8$ subsequent time steps to surpass what it was originally, $\Delta t_0$. In general, $N$ time step halvings, would require $\log_b(2^N)$ steps get back to the original value, using $b$ as the time-step increase factor. Since only one increase in time step was allowed per simulation step, simulations were taking many more timesteps than necessary.

An improved algorithm is described in \cref{alg:trapezoid_improved}. This algorithm maximises the time step given absolute and relative error criteria, ($\epsilon_{\rvar{max}}$, $\upsilon_{\rvar{max}}$), absolute maximal and minimal timesteps, ($\Delta t_{\rvar{max}}$, $\Delta t_{\rvar{min}}$) and number of iterations, $\rvar{iter}_{\rvar{max}}$. It also uses a modulated timestep increase by how far under the absolute accuracy criteria we are. It also fixes a bug that made the error calculation only account for the largest magnitude of a single component of the 3D vectors, rather than the largest norm. The new implementation can be found in the most current version of \href{https://github.com/TarletonGroup/EasyDD/blob/master/src/int_trapezoid.m}{\texttt{int\_trapezoid.m}}.
\begin{algorithm}
    \caption{Improved adaptive timestep algorithm.}
    \label{alg:trapezoid_improved}
    \begin{algorithmic}
        \State $\rvar{Convergent} \gets \rvar{false}$, $\Delta t_{\rvar{valid}} \gets 0$, $\rvar{iter} \gets 0$, $\rvar{flag} \gets \rvar{false}$
        \While{$\rvar{Convergent}$}
        \State Compute $\vec{r}_i^{\rvar{P}}(t + \Delta t)$ and corrector $\vec{r}_i(t + \Delta t)$ from \cref{eq:trapezoid}.
        \State $\Delta \vec{r}_i \gets \vec{r}_i(t + \Delta t) - \vec{r}_i^{\rvar{P}}(t + \Delta t)$
        \State $\vec{\overline{r}}_i \gets \dfrac{\vec{g}_i(\left\{\vec{r}_j(t)\right\}) + \vec{g}_i\left(\left\{\vec{r}_i^{\rvar{P}}(t + \Delta t)\right\}\right)}{2}\Delta t$
        \State $\epsilon \gets \max_i\left(\lVert \Delta \vec{r}_i \rVert \right)$
        \State $\upsilon \gets \max_i\left(\lVert \Delta \vec{r}_i - \vec{\overline{r}}_i \rVert\right)$
        \If{$\epsilon < \epsilon_{\rvar{max}} $ and $\upsilon < \upsilon_{\rvar{max}}$}
        \State $\Delta t_{\textrm{valid}} = \Delta t$
        \State $\gamma \gets 1.2\left(\dfrac{1}{\left[1 + (1.2^{20} - 1) (\epsilon / \epsilon_{\textrm{max}})\right]}\right)^{1/20}$
        \State $\rvar{flag} \gets \rvar{true}$
        \State $\rvar{iter} \gets \rvar{iter} + 1$
        \State $\Delta t \gets \max\left(\gamma \Delta t,\, \Delta t_\rvar{max}\right)$
        \ElsIf{$\rvar{flag} == \rvar{true}$}
        \State $\Delta t \gets \Delta t_{\rvar{valid}}$
        \State $\rvar{iter} \gets \rvar{iter}_\rvar{max}$
        \ElsIf{$\Delta t < \Delta t_\rvar{min}$}
        \State $\Delta t \gets \Delta t / 2$
        \State $\rvar{iter} \gets \rvar{iter} + 1$
        \ElsIf{$\Delta t == \Delta t_\rvar{min}$}
        \State $\rvar{iter} \gets \rvar{iter}_\rvar{max} + 1$
        \Else
        \State $\Delta t \gets \Delta t / 2$
        \EndIf
        \If{$\rvar{iter} > \rvar{iter}_{\rvar{max}}$ or $\Delta t == \Delta t_{\rvar{max}}$}
        \State $\rvar{Convergent} \gets \rvar{true}$
        \EndIf
        \EndWhile
        \State $t \gets t + \Delta t$
        \State Proceed with the rest of the simulation.
    \end{algorithmic}
\end{algorithm}

\Cref{alg:trapezoid_improved} is more computationally expensive per iteration, but can increase the timestep more rapidly, optimally and is more robust than \cref{alg:trapezoid}. The original algorithm also wasn't correctly implemented. Together, these changes let the new version of the software move through the elastic regime of our simulations orders of magnitude times faster than before. Simulations that took hours to days to reach the plastic regime, started doing so in minutes to hours. It also narrowed the gap between different loading conditions, as simulations were now more capable of effectively adjusting the timestep to more efficiently match the error tolerances. However, as discussed in \cref{c:simulations}, there is still a danger of overloading simulations. Fixing the integrator was the first major bottleneck we fixed, allowing us to identify and resolve downstream issues that were previously unkown.

\subsection{Matrix conditioning}
\label{ss:matrix}

A common problem with discrete dislocation dynamics is the tendancy for some networks to have a large degree of variation in the speed of dislocation nodes \cite{bertin2019gpu,ddlab,arsenlis2007enabling}. This is a consequence of the fact that nodes are discretised representations of dislocation lines. As such, nodal motion must be constrained to the motion of dislocations, i.e. nodal movement must be consistent with how a dislocation glides, climbs and cross-slips, accounting for dislocation character and slip system. Moreover, dislocation motion can be assumed to follow a linear, anisotropic, viscous drag model \cite{ddlab},
\begin{align}\label{eq:dragCoef}
    \vec{F}_\textrm{drag}(\vec{x}) & = -\mathcal{B}(\vec{\xi}(\vec{x})) \cdot \vec{v}(\vec{x})\,,
\end{align}
where $\mathcal{B}$ is a tensor constructed from the anisotropic drag that a dislocation node, $\vec{x}$, experiences as a result of its constrained velocity $\vec{v}(\vec{x})$ due to being part of a discritesed segment $\vec{\xi}(\vec{x})$. In simple terms, this means a node can experience very different resitances to moving along its allowed movement vectors.

Different mobility laws can have different parametrisations for their permitted movement vectors. A problem that often plagues traditional mobility laws\footnote{Bruce Bromage, of \cite{bromage2018calculating} has developed a new BCC mobility law, \href{https://github.com/TarletonGroup/EasyDD/blob/master/src/mobbcc_bb1b.m}{\texttt{mobbcc1\_bb.m}} which can be found in the most current version of EasyDD. It solves many of the issues with traditional laws and is our new go-to for BCC materials. Among the things it fixes are: unreasonable cross-slip, pencil glide, and the problematic line-direction parametrisation. The drag matrix, $\mtx{B}$ in \cref{eq:nodeVel}, can still be singular however, so this law also includes the scale-averaged Levenberg-Marquardt regularisation discussed in this chapter.} is the movement of a node along a line segment itself. \Cref{f:lineMovement} shows how a node moving along a line segment does not change its topology, and should therefore have no effect on the energy of the network. Consequently, nodal movement along the line direction must not contribute to the total drag, so the line drag component of the stress tensor, $\mathcal{B}$, should tend to zero. On the other end of the spectrum we have climb, which is orders of magnitude less favourable than glide, so that component of the drag tensor should be much larger than the rest. However, these requirements can make the drag matrix, $\mtx{B}$ in \cref{eq:nodeVel}, singular. Keeping the matrix invertible while simultaneously meeting the physical requirements is a careful balancing act. Moreover, the low drag along the line direction often causes nodal speeds along these vectors to be much higher than other directions, often limiting the integration time step as a result.
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{lineDirMov.pdf}
    \caption{Node movement along dislocation line should have no drag contribution.}
    \label{f:lineMovement}
\end{figure}

In the overdamped regime, $\dfrac{\rvar{d}\vec{v}}{\rvar{d}t}(\vec{x}) \to \vec{0}$, known driving forces (Peach-K\"{o}hler force + segment-segment forces + self-force) and the unkown drag force sum to give,
\begin{align}\label{eq:drivePlusDrag}
    \vec{F}_\rvar{drag}(\vec{x}) + \vec{F}_\rvar{drive}(\vec{x}) = \vec{0}\,.
\end{align}
Substituting \cref{eq:dragCoef} into \cref{eq:drivePlusDrag} and rearranging gives,
\begin{align}\label{eq:nodeMob}
    \mathcal{B}(\vec{\xi}(\vec{x})) \cdot \vec{v}(\vec{x}) & = \vec{F}_\rvar{drive}(\vec{x})\,.
\end{align}
Strictly speaking, solving \cref{eq:nodeMob} requires global knowledge of the network. Fortunately, the discretisation makes it possible to distribute $\mathcal{B}$ along a line segment $i-j$,
\begin{align}\label{eq:dragMatrix}
    \mtx{B}_{ij} & \gets \oint_{C} N_i(\vec{x}) \mathcal{B}(\vec{\xi}(\vec{x})) N_j(\vec{x}) \rvar{d}L\,,
\end{align}
where $C$ is the whole network, $N_i$ and $N_j$ are simply shape functions that interpolate quantities given the relative position of a node to a segment, and $\rvar{d}L$ is the infinitesimal line segment. Thus giving individual expressions for node $i$ connected to node $j$. Using this discretisation and assuming the nodes connected to $i$ are subject to similar conditions, \cref{eq:nodeMob} can be broken up into an approximate expression for the velocity of a single node,
\begin{align}\label{eq:nodeVel}
    \vec{v}_i \approx \left(\sum\limits_j \mtx{B}_{ij}\right)^{-1} \cdot \vec{f}_i,\,
\end{align}
where $i$ is the node in question, $j$ are the nodes it shares segments with, and $\vec{f}_i$ is the local driving force on the node. From now on, we will drop the Einstein notation and refer to the local frame as $\vec{v} = \mtx{B}^{-1} \vec{f}$.

The immediate implication of the orders of magnitude difference in the drag coefficients used to construct $\mathcal{B}$---and therefore $\mtx{B}$---means the condition number of $\mtx{B}$ has the potential to be very large. Given that $\mtx{B}$ is symmetric and therefore normal, its condition number is given by,
\begin{align}\label{eq:conditionNumb}
    \kappa(\mtx{B}) = \dfrac{\lvert \lambda_\rvar{max} \rvert}{\lvert \lambda_\rvar{min} \rvert} \,,
\end{align}
where $\lambda_i$ is the $i$\textsuperscript{th} eigenvalue, and min, max are the minumum and maximum eigenvalues.

The condition number of a matrix codifies how much collinearity exists in its basis. The larger the condition number, the greater its basis' collinearity, and the more sensitive the sytem is to small perturbations. Such systems are said to be ill-conditioned. As a general rule, a condition number, $\kappa(\mtx{A}) = 10^k$, represents a loss of up to $k$ digits of precision on top of what is already lost from arithmetic methods under limited precision \cite{cheney2012numerical}.

True to Murphy's law, $\mtx{B}$, is frequently ill-conditioned---even in the simplest simulations. The way these cases were handled relied on the integrator reducing the timestep enough to sidestep the issue. We call this ``strongly coupled'' behaviour. Strong coupling between functionally independent functions or subroutines is a \emph{very} bad practice in software engineering.

The ``buginess'' of a function $\mathcal{X}$ with a set of $\mathbb{X}$ bugs whose phase space is defined by $\chi(x)$, can be mathematically expressed as, $\forall x \in \mathbb{X}~ \chi(x)$. That is, for all inputs, $x$, some process, $\chi(x)$, produces a set of bugs $\mathbb{X}$. In other words, how buggy a function is, depends on the severity and frequency with which the process, $\chi(x)$, produces bugs, $\mathbb{X}$, for different inputs.

Bugs that evade tests and make it into production code are often either low impact with broad phase spaces, or high impact with narrow phase spaces. The way by which both types of bugs commonly make it into production code differs. Usually, low impact, broad phase space bugs make it into production due to a lack of proper unit testing on ``obvious'' or ``trivial'' cases. High impact, narrow phase space bugs are usually down to rare corner cases such as catastrophic cancellation, loss of precision, division by zero, memory over or underflows, memory leaks, silent errors/crashes due to poor API (Application Programming Interface) design, etc. Both types of bugs can be avoided with a combination of proper software design, and testing protocols. Unfortunately, mitigation strategies go out the window when functions are strongly coupled. We want functions to be independent variables in our process. Just like in probability, phase spaces are multiplicative, and we want their products to be linearly independent. By having strongly coupled functions we make them non-linear, as one function depends on the other to solve its problems.

The way our old mobility functions dealt with ill-conditioned $\mtx{B}$, relied on the integrator picking up the slack. There are several flaws with the approach found in \cref{alg:bTotalOld}, an example of a mobility function with this problem can be found under commit \href{https://github.com/TarletonGroup/EasyDD/blob/65907b022d1fe408fc1b2e5c5ca2bd1797ccae04/mobbcc1.m#L68}{\texttt{65907b0}} under the name \texttt{mobbcc1.m}. Every mobility law in that commit has this issue.
\begin{algorithm}
    \caption[Bad way of avoiding drag matrix inversion singularity.]{Avoiding the inversion of a singular matrix by making $\mtx{B}$ extremely wrong and hoping the integrator error bounds pick it up and decrease the timestep.}
    \label{alg:bTotalOld}
    \begin{algorithmic}
        \State Compute eigenvalue matrix, $\mtx{D} \gets \mtx{P}^{-1} \mtx{B} \mtx{P}$
        \State $\kappa(\mtx{B}) \gets \lvert \lambda_{\rvar{max}} \rvert/ \lvert \lambda_{\rvar{min}} \rvert$
        \If{$\kappa(\mtx{B}) < \kappa_{\rvar{min}}$}
        \State{\Comment{Normalise $\mtx{D}$ and $\vec{f}$ to the maximum eigenvalue.}}
        \State $\mtx{D} \gets \mtx{D} / \lambda_{\rvar{max}}$
        \State $\vec{f} \gets \vec{f} / \lambda_{\rvar{max}}$
        \State{\Comment{Invert $\mtx{D}$ by inverting each non-maximal eigenvalue.}}
        \ForAll{$(\lambda_k \in \mtx{D}) \neq \lambda_{\rvar{max}}$}
        \State{\Comment{$\lambda_\rvar{crit} \to 0$ is some arbitrary threshold for the minimum value of $\lambda_k$.}}
        \If{$\lambda_k > \lambda_{\rvar{crit}}$}
        \State $\lambda_k \gets 1/\lambda_k$
        \Else
        \State{\Comment{The inverse of 0 is not 0.}}
        \State $\lambda_k \gets 0$
        \EndIf
        \EndFor
        \State{\Comment{$\mtx{D}$ is already inverted.}}
        \State $\vec{v} = \mtx{P} \mtx{D} \mtx{P}^{-1} \vec{f}$
        \Else
        \State $\vec{v} = \mtx{B}^{-1} \vec{f}$
        \EndIf
    \end{algorithmic}
\end{algorithm}
There are various problems with \cref{alg:bTotalOld}.

Assuming that an ill-conditioned matrix can be fixed by diagonalising and normalising. This is simply not true, it can help with numerical stability in some algorithms for the solution of linear systems but may not be enough. Perturbing the smallest eigenvalue in a direction such that $\kappa$ improves is a good way to renormalise a system without adding much error. However, perturbations of this type are often not enough when the condition number is too large. Furthermore, one wants to keep the overall behaviour of the system mostly intact, i.e. we want to maintain the relative sizes of the eigenvalues with respect to one another. If other eigenvalues are close in magnitude to the smallest one, the dynamics of the system can very easily change, even a very small perturbation may have undesireable consequences. Not to mention the fact that $\kappa$ may still be too large. In a system as stiff and dynamic as discrete dislocation dynamics, this approach is not robust enough.

The assumption that the case where $\lambda_k < \lambda_{\rvar{crit}}$ for non-maximal eigenvalues is so rare that it won't have much of an effect. Unfortunately, it's common in junctions where nodal mobility is limited, as well as in volumes with large stress gradients. Although to be fair, these are also a consequence of the assumption of locality in \cref{eq:nodeVel}. Moreover, when every non-maximal eigenvalue is smaller than $\lambda_{\rvar{crit}}$, the error is much greater. For every eigenvalue under the threshold of $\lambda_{\rvar{crit}} = 10^{-k}$, the upper bound of the error is $k$ digits in the corresponding mobility component.

Finally, assuming the integrator will not allow obviously wrong solutions because it will fail the error check and reduce the time step is not something to hang our hats on. Erroneous velocities only get caught within the integrator, not when nodal mobility is calculated outside the time integration, which happens in every topological operation where a new node is spawned. Spawning a new node with extremely incorrect velocities can prove to be quite a serious problem, especially if this happens in a region with a high dislocation density.

The regularisation of ill-conditioned systems is an active field of research \cite{regularisation1,regularisation2,regularisation3}. However, they require some knowledge of the matrix structure. Regularlisation is most often applied to high-rank matrices because poorly conditioned, low-rank matrices are often very sensitive to perturbations. In this case, $\mtx{B}$ is positive definite of rank equal to the number of orthogonal crystallographic directions, so it is always of low rank.

One of the most effective regularisations for low rank matrices is dampening the inversion by adding a small value, $c$---called the Levenberg-Marquardt coefficient---to a diagonal element of the matrix. This is roughly equivalent to perturbing the smallest eigenvalue as previously mentioned. If this factor is added the whole diagonal, the inversion can be sufficiently dampened, whilst keeping the main contribution from each basis vector roughly equal in relation to each other. This amounts to adding a small multiple of the identity matrix, $\mtx{I}$, to an ill-conditioned matrix. For a more accurate solution, one can iteratively refine $c$ to find the smallest value that keeps the matrix inversion from blowing up. A good heuristic for the value of the Levenberg-Marquardt coefficient is,
\begin{align}\label{eq:marquardt}
    c = \max\left(\lvert\mtx{A}\rvert\right) \sqrt(\epsilon)\,,
\end{align}
where $\epsilon$ is machine precision and $\lvert A\rvert$ denotes the absolute values of matrix $\mtx{A}$. This ensures the perturbation is scaled to a number that will not underflow during arithmetic operations. Simply put, it ensures the perturbation is small but does not disappear during matrix inversion.

It is worth noting that this renormalisation is a source of error. Adding noise to a system---even if small and along the diagonal---moves the system away from its true behaviour. However, the degree to which it is wrong is many orders of magnitude smaller than what is done in \cref{alg:bTotalOld}. Indeed, \cref{alg:bTotalNew} uses $\pm c $ and both perturbed solutions are averaged to give a much more accurate and simpler solution even under the traditionally problematic scenarios mentioned above. This new algorithm is now present in the mobility laws we now use, such as the most current version of \href{https://github.com/TarletonGroup/EasyDD/blob/372984499dd60136fc7badabd6cee192058d55d9/src/mobbcc_bb1b.m#L199}{\texttt{mobbcc\_bb.m}}.
\begin{algorithm}
    \caption[Dampening the drag matrix inversion singularity.]{Improved regularisation of $\mtx{B}$ by way of perturbing the diagonal.}
    \label{alg:bTotalNew}
    \begin{algorithmic}
        \If{$\lVert \mtx{B} \rVert < \epsilon$}
        \State $\vec{v} \gets \vec{0}$
        \ElsIf{$\lvert \lambda_{\rvar{max}} \rvert/ \lvert \lambda_{\rvar{min}} \rvert > 1/\epsilon$}
        \State $\mtx{B}_+ \gets \mtx{B} + \sqrt{\epsilon} \max(\mtx{\lvert B\rvert}) \mtx{I}$
        \State $\mtx{B}_- \gets \mtx{B} - \sqrt{\epsilon} \max(\mtx{\lvert B\rvert}) \mtx{I}$
        \State $\vec{v}_+ \gets \mtx{B}_+^{-1} \vec{f}$
        \State $\vec{v}_- \gets \mtx{B}_-^{-1} \vec{f}$
        \State $\vec{v} \gets (\vec{v}_- + \vec{v}_+)/2$
        \Else
        \State $\vec{v} \gets \mtx{B}^{-1} \vec{f}$
        \EndIf
    \end{algorithmic}
\end{algorithm}

This change enabled other team members, namely Haiyang Yu and Fengxian Liu to get further with their simulations. Haiyang's nanoindentation simulations have particularly complex dislocation structures with a substantial amount of junction formation; while Fengxian's have large localised stress gradients as a result of inclusions. Both require nodal velocities to be very accurately calculated, else the dislocations move erratically, readily cross slip, and cause massive slowdowns as simulations advance.

\section{Research software engineering}
\label{s:rse}

Many scientists and researchers spare no thought for good software. However, with the presently ongoing SARS-CoV 2 pandemic, the importance of research software has been made clear. Modelling has played a large role in informing government policy in the UK and the world \cite{covidScotland,covidUK1,covidUK2}. With modelling thrust into the spotlight, and the public release of the Imperial College modelling code, which was used to inform so much policy \cite{covidUK2}, \href{https://github.com/mrc-ide/covid-sim}{\texttt{https://github.com/mrc-ide/covid-sim}}, criticism was levied at the lack of care taken to ensure the correctness of the code \cite{natureModelCritique}.

In experimental research, a lot of care is take to ensure the methodoogy used is reproducible, robust, and sound. There are standards, both external and internal, which ensure the quality of the results. But even with these measures in place, there is already a common issue with irreproducibility in science \cite{mede2020replication,randall2018irreproducibility,bolli2015reflections}. In the modern age, software is ubiquitous in the obtention, analysis and communication of scientific research. Anyone hoping to do science will interface with software at some point. This software should therefore be as high quality as possible, \emph{especially} if the science is modelling-based.

Along this vein, one of the primary goals of this project was to develop a competent codebase that non-experts can take advantage of with minimal preparation. There were multiple sub-objectives that help us achieve this.
\begin{enumerate}
    \item Provide a generic framework upon which the software can be modularly expanded with minimal change to the source: prevents increasingly divergent and incompatible source code between researchers.
    \item Provide an easy way to autocomplete inputs with sensible default parameters: prevents the software from crashing when---inevitably---one of the required arguments for a function is undefined.
    \item Improve readability and organisation: makes it easy to identify, fix bugs and know what the code does.
    \item Compartmentalise variables: makes the code more generic and increases usability and readability by reducing the number of function arguments.
    \item Optimise memory and computation: improves computational speed and reduces resource use.
\end{enumerate}

Akin to the Ship of Theseus, it is hard to tell whether the new version of the code is the same code as when this project began. While \href{https://github.com/TarletonGroup/EasyDD}{\texttt{EasyDD v2.0}} is ``complete'', the process of improving the code is ongoing. The amount of initial technical debt since its inception as the infinite-domain discrete dislocation dynamics code, \href{http://micro.stanford.edu/wiki/Main_Page}{\texttt{DDLab}} \cite{ddlab}, plus what it accrued as the Tarleton Group expanded its functionality and turned it into \href{https://github.com/TarletonGroup/EasyDD/tree/65907b022d1fe408fc1b2e5c5ca2bd1797ccae04}{\texttt{EasyDD}}, is quite large and needs to be carefully chipped away so as not to introduce regressions. However, while the quality and capabilities of the code can be greatly improved, there is only so much that can be done without fundamentally redesigning it completely. This is in part the subject of \cref{c:future}.

Other team members have made significant contributions to the release of \href{https://github.com/TarletonGroup/EasyDD}{\texttt{EasyDD v2.0}}. Some of which remain to be added, but should slowly make their way in. Given the code is the amalgam of the research group's work, we will credit other members' contributions where relevant but will leave the details for them to explain.

As this is actively developed research software, the work described herein will continue past the end of this project, for there are still many improvements to be made to the old codebase on top of whatever functionality is added by future researchers.

\subsection{Organisation}

An oft-overlooked aspect of code useability is its organisation. However, as a codebase grows, it becomes more important for code be properly organised and source controlled. This makes it easier to browse, understand, and provides a crucial safey blanket against things going wrong.

\subsubsection{Source control}

Source control is a crucial aspect of industrial software development. It is an industry standard and is the very first thing to set up when starting a software development project. Academia is \emph{very} far behind in this regard. Source control software can perform a wide variety of tasks but among the most important are:
\begin{inparaenum}[\itshape1\upshape)]
    \item provide a history of restore points in case things break;
    \item keep a history of changes, who and when they made them;
    \item lets developers and users make changes independently and request their changes be added to the main code as patches or new versions;
    \item detect conflicts between versions to be merged; and
    \item a robust versioning system integrated to the history of changes.
\end{inparaenum}

With regard to reproducibility, source control is a crucial tool. In both \cref{ss:integrator,ss:matrix} we took advantage of this to provide a specific commit and a file name. Source control can be used in local and remote services via private LAN (Local Area Network), as well as webhosting services such as Gitlab, Bitbucket, or GitHub as in our case. Webhosted services are a popular choice because they simplify and enhance the user and developer experience. For example, the visibility of code can be easily set to
\begin{inparaenum}
    \item private, where only those explicitly given access have read and write access; or
    \item public, where anyone can view the code and propose changes which can then be reviewed and either accepted or rejected by those with write access.
\end{inparaenum}
They also simplify the use of addons and webhooks that can perform myriad tasks like,
\begin{inparaenum}
    \item automated remote testing on different platforms;
    \item code coverage reports (what lines and how many times each of them was hit during automated testing);
    \item automated documentation generation;
    \item automated merge checks; and even
    \item automated encryption/installation key generation.
\end{inparaenum}

LAN-hosted version control software can make use of everything webhosted implementations can\footnote{Although man quality of life improvements such as GUIs (Graphical User Interfaces) and code editor integration might be unaccessible.}. Indeed, LAN-hosting is used for things pertaining to defense and national security, but the use of webhosted services is an easy choice for academia, industry, and private users. In fact, the digital version of this document is found in \href{https://github.com/dcelisgarza/DPhil_Thesis}{\texttt{https://github.com/dcelisgarza/DPhil\_Thesis}}, among many other repositories from recreational and past work. Moreover, whenever this document references a specific file, the digital version directly links to the relevant information (including relevant lines).

Publically available, webhosted source control is a boon for open, reproducible science. It can be used to provide a snapshot of the specific version of the code and data used to generate the results of a manuscript when submitting to a journal. Such platforms can be used to generate funding and copyright, and even enable independendant publishing through the ISBN registry and crowd-sourced peer-review. Through the use of tools such as Jupyter, one can write interactive publications, dubbed ``notebooks''. These notebooks give the power of peer-review back to the community, as well as being great tools for science communication, teaching, and even interactive techincal talks.

In summary, source control is the solution to multiple problems that inhibit scientific progress. On one hand, it provides a safety blanket with many bells and whistles that routinely saves researchers and companies time, money, effort, and heartache. On the other, it democratises scientific research by sidestepping the politics, costs, and ransoming of \emph{publically} funded research that is, at its core, for the benefit of humanity.

\subsubsection{Folder structure}

An ongoing task is improving the folder structure, which in the past was almost non-existant. Having a default place to place inputs, outputs and source code is of great import if a code is to be used by non-experts. It is trivial to do a large quality of life improvement for developers and users.

\subsection{Modularisation}

Modern software practices usually make heavy use of some form of variable encapsulation and code modulirisation to make code safer, simpler to expand, and easier to use. This enables users and developers to take advantage of a set of toolboxes that can be chosen according to their needs without having to directly modify the source code, this concept is called ``modularisation''.

\subsubsection{Encapsulation}\label{ss:encapsulation}

The act of keeping variables neatly stored in a way that fits a purpose is called encapsulation. By encapsulating variables, we greatly improve the usability of software by increasing readability, reducing the risk of human error by reducing the number of things to keep track of, and---if done properly---can even improve performance. How we go about encapsulating code is tightly bound to the design philosophy and programming paradigm of a software product, and the capabilities of the language it is written in. Common programming paradigms are outlined below.
\begin{itemize}
    \item Object-Oriented Programming (OOP): where variables are viewed as objects upon which the logic acts. Functions can take these objects and operate on them. This paradigm keeps related variables as part of a single entity and reduces the number of things users and developers have to keep track of, thus reducing the chance of implementation and user error. There are many advanced concepts and pitfalls in OOP that unecessarily increase complexity, so it is best to apply the KISS (keep it simple, stupid) approach for best results.
    \item Data-Oriven Programming (DDP): where the data dictates the structure of the programme. It can be similar to OOP in some ways, where the object is made to fit the data, not the data made to fit the object. It often leads to more performant code and avoids many of the issues that arise from OOP because it forces developers to really think about how to manage the data.
    \item Functional programming: where everything is a function. There is no state, but also no side effects. This is the strongest form of encapsulation but also the most inflexible.
    \item Procedural programming: this is the most ancient form of programming (that makes use of functions) where there are only functions and variables. It is the form most scientific software packages take, and it is the most difficult to work with both, as a developer and as a user. Even modern computers which can make use of branch prediction and deletion, cache pre-fetching, and compiler optimisations can be hindered by this approach.
\end{itemize}

Procedural programming is what gives us ``spagghetti'' code\footnote{Code with many branches and disparate procedures without a clear purpose. It is difficult to work with as a user and developer. It also tends to interfere with branch prediction and deletion, as well as many other compiler optimisations.}. Unfortunately, education in programming or software engineering in science is sorely lacking. As a result, a large amount of scientific software tends to be written in this way (particularly legacy software). Thus, we strive to separate ourselves from it as much as possible.

Functional programming is very useful where safety and fault-tolerance is a concern. It sees a lot of use in telecommunications, social media, cryptography, finance \cite{haskell,functionalProg}, but usually as part of a solution. It is however, impractical for most applications.

Object-oriented programming can be very useful if used properly. But it has the potential to unecessarily increase complexity and reduce performance. Fortunately, \mintinline{matlab}{MATLAB} does not truly have object-oriented capabilities, so it forces either a procedural or data-driven approach. This often leads to software being written in a procedural manner.

However, we can take a data driven approach and leverage \mintinline{matlab}{struct} to simplify the process. These are more of an ordered dictionary (Hash table), than a true object, but they are what we have available. Converting the code to use them is an ongoing process, but they have so far allowed us to make use of generic functions for different mobility functions, loading conditions, and various auxiliary variables. All of which have a simplifying effect on the code without sacrifing performance. While this project pioneered this task, Daniel Hortelano-Roig has been doing much in the way of increasing its scope. His changes already greatly simplify the code, and will slowly be ported over to the main branch.

\subsubsection{Generic functions}

Generic functions are implementation-agnostic functions that can be given by the user as any other variable. \mintinline{matlab}{MATLAB} has an old method that enables generic programming using \mintinline{matlab}{feval()}, where the first argument is the name of the file whose function is being called, and subsequent arguments are passed on to the function. However, this has some limitations, mainly their performance, and function handles are now preferred, e.g.
\begin{minted}[frame=lines, linenos]{matlab}
    % myFunction is defined in 'myFunction.m' and is called 
    % like so myFunction(foo, bar)
    genericFunction = 'myFunction.m'; 
    foo = 1;
    bar = 2;
    feval('genericFunction', foo, bar);
\end{minted}
as opposed to,
\begin{minted}[frame=lines, linenos]{matlab}
    % myFunction is defined in 'myFunction.m' and is called 
    % like so myFunction(foo, bar)
    genericFunction = @myFunction; 
    foo = 1;
    bar = 2;
    genericFunction(foo, bar);
\end{minted}
which are not dissimilar, but the latter is more performant and easier to follow. Generic functions were already in use for dislocation mobility, but there are now functions for calculating numeric and analytic tractions (see \cref{c:tractions}), loading conditions (multi-stage loading, constant loading, cyclic loading), post-processing functions, boundary conditions, and various miscelaneous functions that support different simulation types.

The main advantage of generic functions is reducing the need for direct source code modification. Changing source code, especially when there are no automated tests to speak of, is a dangerous proposition. It greatly increases the probability of introducing regressions (new bugs) and code divergeance between researchers. Regressions are always problematic, but code divergeance is a big issue that still affects the Tarleton Group. If experts within the same research group find it difficult to incorporate each others' additions to our work, there is little hope for the general user.

The introduction of regressions makes it so code must be thoroughly and exhaustively tested before merging two people's work. The use of generic functions makes it so if there is a problem with a new addition, the problem is localised and can be quickly identified and fixed because it is isolated. We can be sure the changes are only found in the new piece of code. And since it is generic, we can choose whether to use it by changing an input file rather than directly altering the source code. If instead we were to directly modify the source code, we would need to account for new inputs, outputs and perhaps create new branching behaviour. Doing so every time new functionality is added will lead to an explosion of complexity that spaggettifies code, making it difficult to work with and innefficient to run.

\subsection{Optimisation}
\renewcommand{\epigraphflush}{flushright}
\renewcommand{\textflush}{flushright}
\setlength{\epigraphwidth}{0.75\linewidth}
\epigraph{We \emph{should} forget about small efficiencies, say about 97\% of the time: pre-mature optimisation is the root of all evil. Yet we should not pass up our opportunities in that critical 3\%.}{\textit{--- Donald E. Knuth \cite[p.~268]{knuth1974structured}}}

\subsubsection{Spurious memory allocation}

\mintinline{matlab}{MATLAB} is infamous for the way it will happily dynamically grow an array when going out of bounds. This \emph{terrible} practice is even fundmental to the operation of \texttt{EasyDD}. However, changing this fundamental part of the codebase requires a complete rewrite and not deemed worthwhile, there are worse bottlenecks. However, there were many instances of memory reallocated or arrays grown every cycle that were completely unecessary.

For example, the amount of memory unecessarily reallocated in the function that couples DDD and FEM was $12(N_x + 1)(N_y + 1)(N_z + 1)$ double precision floating point numbers, where $N_i$ is the number of elements in dimension $i$ of the finite domain. The cubic scaling is a performance killer at larger mesh sizes. In a simulation with hundreds or thousands of steps and a mesh with a few thousand FE nodes, the cost added up.

It is not only the cost of allocating, but also the cost of garbage collection (GC) \cite{hanson1990fast} i.e. automatically freeing memory based on a set of criteria can be significantly higher. Timing the equivalent of allocating a $20 \times 20 \times 20$ mesh in \mintinline{matlab}{MATLAB 2020b} takes on the order of \SI{70}{\micro\sec} (for x86 CPU architectures), which is in-line to what it costs in \mintinline{C}{C}. Deallocating memory takes about $5 \times$ longer. In a function that otherwise takes a few \si{\milli\sec}, adding another \SI{400}{\micro\sec} of spurious allocation is a significant overhead that can be easily remedied.


\subsubsection{Finite element optimisation}

The use of sparse arrays is a must when working with finite elements, where matrices often take structured sparse forms will significantly reduce memory footprint and computational expense. The code already used these special data structures and Cholesky factorisation for faster solution of linear systems. However, \mintinline{matlab}{MATLAB} has a special form of it that exploits sparsity. This reduced the time required for the factorisation by around $50\times$, reduced the memory footprint of the final sparse arrays by a similar amount, and reduced the memory footprint of the actual factorisation procedure by approximately $10 \times$. The whole point of this factorisation is to make solving linear systems faster, in this case the calculation of the corrective displacements, $\vec{\hat{u}}$ on the free boundaries, $S_U$ of freedom as a result of the forces acting upon them,
\begin{align}
    \vec{\hat{u}} = \mtx{K}^{-1} \vec{f}\quad \text{on } S_U\,.
\end{align}

It improved solution speed by a factor of 3, thus reducing the cost of coupling DDD to FEM to be dominated by the computation of dislocation-induced tractions and displacements.

The final two optimisations also deal with memory. The first, removes unecessary allocations in the calculation of the global stiffness matrix and the introduction of a reduced stiffness matrix such that it only includes relevant degrees of freedom.

In aggregate these changes have given us the ability to use $15 \times$ more nodes as well as letting us build meshes over two orders of magnitude faster, mostly down to improved memory access patterns inside the mesh building loops.

\subsection{Misc quality of life improvements}\label{s:qol}

Formerly, initialising a simulation would involve a series of non-obvious steps, particularly when initially compiling the C and CUDA code used for accelerating simulations. This is now done automatically on a need-to-compile basis and has a fallback in case no \mintinline{CUDA}{CUDA} compiler is found.

There was also a recurring problem with simulations suddenly failing when a function was called with an undefined argument. This extremely common scenario is unbecoming of good software, especially if the failure happened a few minutes after starting a simulation. This has been remedied with the automatic calculation of a set of reasonable defaults for each and every undefined variable, aside from the arrays defining a dislocation network. Any new developments get added to this very simple, yet much needed script.

\section{Conclusions}

Scientific modelling is inherently an interdisciplinary skill. EasyDD is now faster, more efficient and more capable than ever before, in large part thanks to the work---ongoing or otherwise---described within this chapter. Subsequent chapters explore aspects more specific to discrete dislocation dynamics. However, it is important not to overlook the work that enables the obtention of results. Modern academia deems the final result to be paramount. But what is a result if it cannot be scrutinised because nobody can understand how it came about? Moreover, by developing good software, we allow others to partake in the obtention of results. We deem this to be one of the main contributions of the present project. We feel safe our ability to claim that we now have the capacity for providing more faithful recreations of reality in a less user-unfriendly way. And without so much as a glancing touch on dislocation dynamics.
\savearabiccounter
% 2963 edited
% 5135 words