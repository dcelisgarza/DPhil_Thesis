\chapter{Improved tolopogical operations}
\label{c:topology}

In 2D, dislocations are parametrised as points on a surface. Therefore aside from annihilation, interactions with grain boundaries \cite{grain_size_eff1, grain_size_eff2}, and perhaps the creation of superdislocations, topological changes in the dislocation network can be mostly ignored. However, in 3D, dislocations are parametrised as lines in a volume. This gives rise to extra complexities that result from multiple dislocations coming into contact with one another. These reactions ultimately lead to the formation of many secondary structures with their own properties that affect the rest of the network.

The local dislocation density at these interaction sites, particularly arround sessile (immobile) junctions, tends to increase with time. Therefore so do the local stress gradients, and with increasing stress gradients come increasing velocity gradients, which lead to global decreases in timestep. But a smaller timestep is not the only consequence, increases in dislocation density also mean higher probabilities of dislocation-dislocation interactions, so the phenomenon is autocatalytic and self-perpetuating. So properly accounting for these changes is of the utmost importance, particularly for simulations with high dislocation densities such as nanoindentation. This chapter details these improvements.

\section{Collision}\label{s:collision}

\subsection{Hinges}\label{ss:hinges}
The lowest hanging fruit and most obvious problem were collisions that share a lot of similarities with the remeshing criteria that checks for a minimum area enclosed between two segments connected by a single node, as shown in \cref{f:hinge}. A single dislocation hinge can lead to three different final topologies, the conditions for which can be simultaneously met in any combination
\begin{enumerate}
    \item Remeshing to eliminate the middle node because the area, $a$, enclosed by the triangle created by segments $s_1$ and $s_2$ is less than the minimum allowable area, $a_{\rvar{min}}$.
    \item Colliding $s_1$ into $s_2$ if the minimum distance between the non-hinge node of $s_1$ (the node $s_1$ does not share with $s_2$) and segment $s_2$ is less than the collision distance, $l_{\rvar{col}}$.
    \item Colliding $s_2$ into $s_1$ if the minimum distance between the non-hinge node of $s_2$ and segment $s_1$ is less than the collision distance, $l_{\rvar{col}}$.
\end{enumerate}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{hinge.pdf}
    \caption[A single dislocation hinge can lead to three different final topologies.]{A single dislocation hinge can lead to three different final topologies, the conditions for which can be simultaneously met in any combination.}
    \label{f:hinge}
\end{figure}

The combination in which these conditions can be met, and the order in which they are checked will have drastic downstream consequences (see \cref{s:nonCommutativity}). \emph{Especially} at high dislocation densities where multiple collisions happen at every timestep. This had not been an issue in prior to fixing the integrator and matrix inversion, but simulations could now advance to a point where this was having a significant impact, particularly in Haiyang Yu's nanoindentation simulations and the microtensile simulations in \cref{s:nickelTensile}.

This was a multi-stage fix, where each fix progressively uncovered more and more shortcomings resulting from the unavoidable coupling between topological operations. These progressive fixes are detailed in \cref{s:nonCommutativity}. Here we will only focus on problems with collisions.

The first fix involved prioritising hinge collisions above others, which we call two-line collisions. This meant detecting and forming the new connection. The rationale for this is that a segments in a hinge are close enough to and connected with each other at a node already that they will interact more strongly than two unconnected segments approaching one another.

The second was much more subtle. \Cref{f:hinge} has two options for colliding a hinge. And they both result in differently sized segments $l_1 < l_2$. The one the code calculated depended on which segment, $s_1$ or $s_2$ it came across. If $s_1$ was first, then the answer would be $l_1$, otherwise it would be $l_2$. At the very least, the answer should be the same for a given network regardless of how it is represented. So the first step is to ensure one of these is always chosen, but once this is done it is a simple case of calculating the distance from the short segment to the long one.

Remeshing used to be called after finding a \emph{single} collision. This worked well when the number of collisions at a given time step was small. The problems with this are detailed in \cref{s:nonCommutativity}. But before moving on to that, it became apparent that there was another issue with the collision detection.

\subsection{Collision distance}\label{ss:collisionDistance}

In the same vein as \cref{ss:hinges}, two-line collisions were being improperly prioritised. They occurred arbitrarily as the code came across them. Again yielding different answers for different representations of the same network. Only this time it had to do with the collision radius as seen in \cref{f:collisionRadius}.
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{collisionDist.pdf}
    \caption[Dislocation segments inside different collision radii.]{Multiple dislocation segments inside the same collision radius.}
    \label{f:collisionRadius}
\end{figure}

The actual algorithm is more sophisticated because it accounts for both complete segments and finds the two points along them with the minimum distance between each other. But the problem was that as soon as a viable collision was found, the function returned. So in a scenario like \cref{f:collisionRadius}, if the code were looking for a collision on $s_1$ and it came across $s_3$ first it would flag that as the collision disregarding the fact that not only was $s_2$ closer, but $s_2$ and $s_3$ are also closer. The same fix had to be applied to hinge collisions.

Despite the fact that checking all possible collisions for the minimum distance is a more expensive task, it results in faster simulations because the resulting structures are guaranteed to either produce the shortest segments, which can be cleaned up by remeshing, or the highest energy structures, which can be cleaned up by separation. Otherwise the structures often end up in a no-man's land which is energetically unfavourable to separate, or cannot be remeshed; leading to an accumulation of highly immobile, highly energetic structures that cause more and more collisions as simulations advance and create sharp stress gradients that cause the timestep to drop significantly, preventing simulations from advancing very much after hitting a critical point.

\section{Non-commutativity of topological changes}\label{s:nonCommutativity}
% todo #13
For a given iteration the time-evolution of the network used to be performed before any topological changes. This was very sub-optimal for a couple of reasons.

The fundamental reason is that by doing so, the time evolution erroneously accounts for complex structures that should have dissipated into lower energy ones as per the quasi-static principle our model is based on. In other words, high energy structures in the network mean large stress gradients. These break the assumption that a node and its neighbours experience similar conditions. These locality assumptions lead to \label{eq:nodeVel}, which is what all our mobility laws are based on.

The practical reason is that by leaving these structures around when they should have been removed, the computational complexity increases by virtue of having more nodes and segments that would otherwise not be there.

Moreover, the order in which topological changes were carried out was also very sub-optimal. It used to be the case that separation, collision and remeshing were performed in that order. This would result in nodes being separated only to immediately collide again. Furthermore, remeshing at the end meant having segments that should not have been there in the first place, or not having segments that should have. Meaning the network was not properly discretised before collisions and separations were performed, leading to improper structures.

One of the larger problems with higher dislocation densities is the computational complexity of the separation function. Its role is to take high energy, highly connected structures and break them apart into lower energy, lower connection ones. Separation is a very poorly scaling operation $\mathcal{O}\left(\sum\limits_{i}\sum\limits_{j=2}^{\left\lfloor C_i/2 \right\rfloor} \dfrac{C_i!}{(C_i-j)! j!}\right)$. Where the sum over $i$ is done over the nodes with more connections than is allowed; $C_i$ is the number of connections for node $i$; and $j$ is the number connections $C_i$ connections can be split into. Mirror symmetries where $2j = C_i$ half the number of possible splitting configurations for that case. However at best, this puts the lower bound at approximately Sterling's factorial formula $N! \approx \mathcal{\Theta}(N\log{N})$ for large $N$. All of these configurations must be generated---an $\mathcal{O}(M)$ operation where $M$ are the number of nodes to be added---in order to check their power dissipation. Unfortunately, this is a relatively expensive operation because the code dynamically expands the relevant arrays. The nodal velocities and segment forces must then be calcualted for every configuration. Whichever configuration represents the and highest energy dissipation is picked as the final configuration. Locality is assumed, so only the forces and mobilities of the segments directly involved in the split are calculated. It is therefore extremely important that the number of separations each step be kept to a minimum.

The solution may appear to be as simple as calling separation after resolving a single collision, and looping through all collisions in a given timestep. However, this can cause collisions to be undone, leading to an infinite loop. If this happens, and there is no change in the number of nodes and links pre- and post-collision-separation, the offending segments are stored in a buffer whose entries are skipped by the collision detection. Once the segments have been added to the buffer, the iteration is run again without updating the network, and the collision detection will skip any pairs of colliding segments in the buffer. Once a successful collision-separation loop is performed, the buffers are deleted and the cycle repeats itself until there are no more collisions that cause the network to change. This means collisions can be checked more than once, but in keeping with the quasi-static conditions, ensures all first-order\footnote{A true minimal energy structure would require all collisions to be performed and a full traversal of every splitting mode tree.} energetically favourable topological operations are performed every timestep while preventing infinite loops. This, together with the fixes made to the collision detection, ensures subsequent iterations of the collision-separation loop utilise minimal energy structures. An added benefit is that it also keeps the number of possible separations to a minimum because single collisions get resolved every iteration.

It is worth noting that nodes labelled as immobile (an artificial construct to mimic pinning) are not split by separation. Which means they sometimes become very highly connected and as a result can create an impassable obstacle for other nodes. As a result, they get `unpinned' when they reach a critical level of connectivity. Separation is performed after this check to break them apart, and also because nodes may need separating despite there being no collisions during the timestep.

The role of remeshing is to ensure the resolution of the network is appropriate. It does so by ensuring there is an appropriate number and distribution of nodes throughout the network to keep discretisation accuracy high, whilst minimising computational cost. Therefore, the final piece of the puzzle was remeshing the network pre- and post-collision-separation loop, so the next iteration starts with a clean network.

The new order of topological operations in a single iteration is as follows,
\begin{enumerate}
    \item remesh,
    \item collision-separation loop,
    \item mobilise highly connected immobile nodes,
    \item separation,
    \item remesh.
\end{enumerate}
For a given iteration, the first remeshing is called after the network is evolved in time, as the nodes can move into positions that necessitate a remeshing to keep the resolution appropriate; it is then called again at the end of the iteration because collision and separation could have created structures that don't meet the network resolution criteria.

\section{Conclusions}

Discrete dislocation dynamics is a stiff and chaotic dynamical system. It is very sensitive to small changes, as we will see in \cref{c:tractions}. Having a correct topology is of the utmost importance in this regard. Something as small as a collision that was not performed when it should have, can have dramatic downstream consequences. As dislocation density increases, the degree of correctness when resolving their interactions increases in importance. Not only does it increase the accuracy of a simulation, but improves its running speed.

The increase in accuracy is obvious; detecting all the collsions that should be detected; resolving them in a manner more keeping with the quasi-static assumption yielding the mobility law; and ensuring the network resolution is what it should be; make for more accurate simulations.

Part of the increase in running speed is down to the fact that quasi-static conditions are better met than before, so the timsetep can be greater. The other huge benefit to running speed is down to the asymptotic scaling and high proportionality constant of the algorithms we use; $\mathcal{O}(N^2)$ for seg-seg forces and collisions; $\mathcal{O}(C_i!)$ for a single separation; $\mathcal{O}(M N)$ for tractions. Where $N$ is the number of segments, $C_i$ the number of connections, and $M$ the number of surface elements. Simply put, the fewer things to calculate have a disproportionately large impact on how many operations need to be performed every time any of these functions is run.

With these improvements, simulations that used to be computationally intractible within a reasonable timeframe are now very doable. Large, complex simulations with high dislocation densities are still slow by virtue of how topologically active they are. They are simultaneously more accurate and faster by multiple orders of magnitude (sometimes 5 or more) than before.
\savearabiccounter
%1864